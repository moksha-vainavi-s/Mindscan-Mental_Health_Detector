# -*- coding: utf-8 -*-
"""MENATL HEALTH DETECTOR_FINAL PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aEdLzLWVA1ch5Z8GNlrEk6d9vV43woZm
"""

import pandas as pd
import numpy as np

df=pd.read_csv('/content/Depression.csv')
df

df.head()

df.tail()

df.info()

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

df.dtypes

df.shape

df.describe()

df.columns

df.duplicated().sum()

df

df.drop_duplicates(inplace=True)

df

from sklearn.preprocessing import LabelEncoder
# Encode target
le_target = LabelEncoder()
df['Depression_Label'] = le_target.fit_transform(df['Depression State'].str.strip())

# Strip tabs, numbers, and spaces
df['Depression State'] = df['Depression State'].str.strip()         # remove spaces/tabs
df['Depression State'] = df['Depression State'].str.replace(r'^\d+\s*', '', regex=True)  # remove numbers at start

# Check cleaned unique values
print(df['Depression State'].unique())

import matplotlib.pyplot as plt
import seaborn as sns

#Count plot
plt.figure(figsize=(8,6))
sns.countplot(x='Depression State',data=df, palette="Set2")
plt.title("Count of Depression States")
plt.show()

#Grouped bar chart
plt.figure(figsize=(10,6))
sns.countplot(x='Sleep', hue='Depression State', data=df)
plt.title("Sleep frequency by Depression State")
plt.show()

#Pair plot
sns.pairplot(df[['Sleep', 'Appetite', 'Fatigue', 'Worthlessness', 'Depression State']], hue='Depression State')
plt.show()

#Boxplot
plt.figure(figsize=(12,6))
sns.boxplot(x='Depression State', y='Fatigue', data=df)
plt.title("Fatigue levels across Depression States")
plt.show()

#Correlation Heatmap
plt.figure(figsize=(14,10))
sns.heatmap(df.select_dtypes(include=['int64','float64']).corr(),
            annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import RandomOverSampler

# -------------------- Clean rare classes --------------------
counts = df['Depression State'].value_counts()
rare_classes = counts[counts < 2].index
df = df[~df['Depression State'].isin(rare_classes)]

le_target = LabelEncoder()
df['Depression_Label'] = le_target.fit_transform(df['Depression State'])

# -------------------- Features and target --------------------
trained_features = [
    'Sleep','Appetite','Interest','Fatigue','Worthlessness','Concentration',
    'Agitation','Suicidal Ideation','Sleep Disturbance','Aggression',
    'Panic Attacks','Hopelessness','Restlessness','Low Energy'
]
X = df[trained_features]
y = df['Depression_Label']

# -------------------- Train-test split --------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# -------------------- Handle class imbalance --------------------
ros = RandomOverSampler(random_state=42)
X_train_res, y_train_res = ros.fit_resample(X_train, y_train)

# -------------------- Scale features --------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

# -------------------- Models --------------------
models = {
    "Logistic Regression": LogisticRegression(multi_class='multinomial', max_iter=500),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
    "KNN": KNeighborsClassifier()
}

#Scatter plot (Logistic Regression)
feature = X.columns[0] # For sigmoid plot: pick one numerical feature (first column after encoding)

x_vals = np.linspace(-10,10,100)
sigmoid = 1 / (1 + np.exp(-x_vals))
y_vals = sigmoid + np.random.normal(0, 0.5, len(sigmoid))

# Scatter + sigmoid curve
plt.figure(figsize=(8, 6))
plt.scatter(x_vals, y_vals, color="black", label="Feature", alpha=0.6)
plt.plot(x_vals, sigmoid , color="red", linewidth=2, label="Logistic Sigmoid Curve")

plt.xlabel("Feature Values")
plt.ylabel("Target")
plt.title("Logistic Regression Sigmoid Curve")
plt.legend()
plt.show()

from sklearn import tree
dt_model = models["Decision Tree"]
dt_model.fit(X_train, y_train)
y_pred = dt_model.predict(X_test)

plt.figure(figsize=(12, 10))
tree.plot_tree(
    dt_model,
    feature_names=X.columns,
    class_names=[str(cls) for cls in dt_model.classes_],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.show()

# Feature importance
rf=RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
importances = pd.Series(rf.feature_importances_, index=X.columns)
importances.sort_values().plot(kind='barh')
plt.title("Feature Importances")
plt.show()

from IPython.display import display, Markdown

display(Markdown("# ACCURACY SUMMARY"))

# -------------------- Train and Evaluate --------------------
accuracies = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train_res)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    accuracies[name] = acc
    print(f"{name} Accuracy: {acc:.2f}")

# -------------------- Select Best Model --------------------
best_model_name = max(accuracies, key=accuracies.get)
best_model = models[best_model_name]
print(f"‚úÖ Best Model: {best_model_name} with Accuracy = {accuracies[best_model_name]:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred))
print("-" * 60)

import joblib
joblib.dump(best_model, "mentalhealth_model.pkl")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(le_target, "label_encoder.pkl")

from IPython.display import display, Markdown

display(Markdown("# STREAMLIT"))

from google.colab import drive
drive.mount('/content/drive')

!pip install streamlit

!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlitapp.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import joblib
# 
# st.set_page_config(page_title="MindScan:Mental Health Detector",layout="centered",initial_sidebar_state="auto")
# 
# # -------------------- Background --------------------
# page_bg = f"""
# <style>
# [data-testid="stAppViewContainer"] {{
#     background-image: url("https://slidescorner.com/wp-content/uploads/2022/12/01-Mental-Health-Free-PPT-Backgrounds-by-SlidesCorner.com_.jpg");
#     background-size: cover;
#     background-position: center;
#     background-repeat: no-repeat;
# }}
# [data-testid="stHeader"] {{
#     background: rgba(0,0,0,0);
# }}
# </style>
# """
# st.markdown(page_bg, unsafe_allow_html=True)
# 
# st.markdown(
#     """
#     <h1 style='text-align: center;'>
#         üß† MindScan: Mental Health Detector
#     </h1>
#     """,
#     unsafe_allow_html=True
# )
# 
# # -------------------- Load Dataset --------------------
# data_path = "/content/Depression.csv"
# df = pd.read_csv(data_path)
# 
# # -------------------- Load Model Artifacts --------------------
# @st.cache_resource
# def load_artifacts():
#     best_model = joblib.load("mentalhealth_model.pkl")
#     scaler = joblib.load("scaler.pkl")
#     le_target = joblib.load("label_encoder.pkl")
#     return best_model, scaler, le_target
# 
# # -------------------- Model Accuracies --------------------
# accuracies = {
#     "Decision Tree": 0.48,
#     "Random Forest": 0.48,
#     "KNN": 0.43,
#     "Logistic Regression": 0.37
# }
# 
# best_model, scaler, le_target = load_artifacts()
# 
# 
# best_model_name = max(accuracies, key=accuracies.get)
# 
# # -------------------- Feature Importance --------------------
# trained_features = [
#     'Sleep','Appetite','Interest','Fatigue','Worthlessness','Concentration',
#     'Agitation','Suicidal Ideation','Sleep Disturbance','Aggression',
#     'Panic Attacks','Hopelessness','Restlessness','Low Energy'
# ]
# 
# feature_options = {
#     "Sleep": ["Never","Always","Often","Rarely","Sometimes","Not at all"],      # 1‚Äì6
#     "Appetite": ["Never","Always","Often","Rarely","Sometimes"],                # 1‚Äì5
#     "Interest": ["Never","Always","Often","Rarely","Sometimes"],                # 1‚Äì5
#     "Fatigue": ["Never","Always","Often","Rarely","Sometimes","Not at all"],   # 1‚Äì6
#     "Worthlessness": ["Never","Always","Often","Rarely","Sometimes","Not at all"], # 1‚Äì6
#     "Concentration": ["Never","Always","Often","Rarely","Sometimes"],           # 1‚Äì5
#     "Agitation": ["Never","Always","Often","Rarely","Sometimes","Not at all"], # 1‚Äì6
#     "Suicidal Ideation": ["Never","Always","Often","Rarely","Sometimes","Not at all"], # 1‚Äì6
#     "Sleep Disturbance": ["Never","Always","Often","Rarely","Sometimes"],       # 1‚Äì5
#     "Aggression": ["Never","Always","Often","Rarely","Sometimes","Not at all"], # 1‚Äì6
#     "Panic Attacks": ["Never","Always","Often","Rarely","Sometimes","Not at all"], # 1‚Äì6
#     "Hopelessness": ["Never","Always","Often","Rarely","Sometimes","Not at all"], # 1‚Äì6
#     "Restlessness": ["Never","Always","Often","Rarely","Sometimes","Not at all"], # 1‚Äì6
#     "Low Energy": ["Never","Always","Often","Rarely","Sometimes","Not at all"] # 1‚Äì6
# }
# 
# # -------------------- Prediction Section --------------------
# st.subheader("üéõÔ∏è Predict Mental Health State")
# 
# option_mapping = {
#     "Never": 1,
#     "Always": 2,
#     "Often": 3,
#     "Rarely": 4,
#     "Sometimes": 5,
#     "Not at all": 6
# }
# 
# inputs = {}
# with st.form("prediction_form"):
#     st.markdown("#### üìù Fill out your responses")
# 
#     # Split into 2 columns for better layout
#     cols = st.columns(2)
# 
#     for i, feature in enumerate(trained_features):
#         if df[feature].max() <= 5:
#             options = ["Never", "Always", "Often", "Rarely", "Sometimes"]
#         else:
#             options = ["Never", "Always", "Often", "Rarely", "Sometimes", "Not at all"]
# 
#         with cols[i % 2]:  # alternate features between 2 columns
#             choice = st.selectbox(f"{feature}", options=options, index=0, key=feature)
#             inputs[feature] = option_mapping[choice]
# 
#     submitted = st.form_submit_button("üîÆ Predict Depression State")
# 
# if submitted:
#     input_df = pd.DataFrame([inputs])
#     input_scaled = scaler.transform(input_df)
# 
#     pred_encoded = best_model.predict(input_scaled)[0]
#     pred_label = le_target.inverse_transform([pred_encoded])[0]
# 
#     pred_probs = best_model.predict_proba(input_scaled)[0]
#     class_labels = le_target.classes_
#     prob_df = pd.DataFrame({"Depression State": class_labels, "Probability": pred_probs})
#     prob_df = prob_df.sort_values("Probability", ascending=False)
# 
#     # Show prediction in card
#     st.markdown(
#         f"""
#         <div style="
#             background-color:#e8f5e9;
#             padding:20px;
#             border-radius:15px;
#             box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
#             text-align:center;
#         ">
#             <h3 style="color:#2e7d32;">Predicted State</h3>
#             <p style="font-size:28px; font-weight:bold; margin:0;">{pred_label}</p>
#         </div>
#         """,
#         unsafe_allow_html=True
#     )
# 
#     # Show probability table nicely
#     st.markdown("#### üéØ Prediction Probabilities")
#     st.dataframe(prob_df.style.background_gradient(cmap="Blues"))
# 
# # Strip tabs, numbers, and spaces
# df['Depression State'] = df['Depression State'].str.strip()         # remove spaces/tabs
# df['Depression State'] = df['Depression State'].str.replace(r'^\d+\s*', '', regex=True)  # remove numbers at start
# 
# # Check cleaned unique values
# print(df['Depression State'].unique())
# 
# # -------------------- EDA Section --------------------
# st.subheader("üìä Exploratory Data Analysis")
# 
# def plot_in_card(fig, title=""):
#     st.markdown(
#         f"""
#         <div style="
#             background-color:#ffffff;
#             padding:15px;
#             border-radius:15px;
#             box-shadow: 2px 2px 10px rgba(0,0,0,0.08);
#             margin-bottom:20px;
#         ">
#         <h4 style="color:#444;">{title}</h4>
#         </div>
#         """,
#         unsafe_allow_html=True
#     )
#     st.pyplot(fig)
#     plt.close(fig)
# 
# with st.expander("üìå Count Plot", expanded=True):
#     fig, ax = plt.subplots(figsize=(6,4))
#     sns.countplot(x='Depression State', data=df, palette="Set2", ax=ax)
#     ax.set_title("Count of Depression States")
#     plot_in_card(fig, "Count of Depression States")
# 
# with st.expander("üìåGrouped Bar Chart"):
#     fig, ax = plt.subplots(figsize=(6,4))
#     sns.countplot(x='Sleep', hue='Depression State', data=df, ax=ax)
#     ax.set_title("Sleep frequency by Depression State")
#     plot_in_card(fig, "Sleep frequency by Depression State")
# 
# with st.expander("üìå Pairplot"):
#     pair_cols = ['Sleep', 'Appetite', 'Fatigue', 'Worthlessness', 'Depression State']
#     pair_grid = sns.pairplot(df[pair_cols], hue='Depression State', height=2.2, aspect=1.1)
#     plot_in_card(pair_grid.fig, "Pairplot of Key Features")
# 
# with st.expander("üìå Boxplot"):
#     fig, ax = plt.subplots(figsize=(6,4))
#     sns.boxplot(x='Depression State', y='Fatigue', data=df, ax=ax, palette="Set3")
#     ax.set_title("Fatigue levels across Depression States")
#     plot_in_card(fig, "Fatigue levels across Depression States")
# 
# with st.expander("üìå Correlation Heatmap"):
#     numeric_df = df.select_dtypes(include=['int64','float64'])
#     fig, ax = plt.subplots(figsize=(10,7))
#     sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm", fmt=".2f", ax=ax)
#     ax.set_title("Correlation Heatmap")
#     plot_in_card(fig, "Correlation Heatmap")
# 
# 
# # --------------------  Accuracies --------------------
# st.subheader("üìà Model Accuracies")
# 
# # Sort models by accuracy (descending order)
# sorted_accuracies = dict(sorted(accuracies.items(), key=lambda x: x[1], reverse=True))
# 
# # Make first column wide for best model, rest smaller
# cols = st.columns([2] + [1] * (len(sorted_accuracies) - 1))
# 
# for i, (name, acc) in enumerate(sorted_accuracies.items()):
#     with cols[i]:
#         if name == best_model_name:  # Bigger card for best model
#             st.markdown(
#                 f"""
#                 <div style="
#                     background-color:#e8f5e9;
#                     padding:35px;
#                     border-radius:20px;
#                     box-shadow: 2px 4px 12px rgba(0,0,0,0.15);
#                     text-align:center;
#                     height:180px;  /* üîë Fixed height */
#                     display:flex;
#                     flex-direction:column;
#                     justify-content:center;
#                 ">
#                     <h3 style="color:#2e7d32; font-size:26px;">{name}</h3>
#                     <p style="font-size:40px; font-weight:bold; margin:0; color:#2e7d32;">
#                         {acc:.2f}
#                     </p>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
#         else:  # Smaller cards
#             st.markdown(
#                 f"""
#                 <div style="
#                     background-color:#f9f9f9;
#                     padding:15px;
#                     border-radius:12px;
#                     box-shadow: 1px 2px 6px rgba(0,0,0,0.1);
#                     text-align:center;
#                     height:150px;  /* üîë Fixed height */
#                     display:flex;
#                     flex-direction:column;
#                     justify-content:center;
#                 ">
#                     <h4 style="color:#333; font-size:20px;">{name}</h4>
#                     <p style="font-size:24px; font-weight:bold; margin:0; color:#4CAF50;">
#                         {acc:.2f}
#                     </p>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
# 
# # Highlight best model below (optional)
# st.markdown(
#     f"""
#     <div style="
#         background-color:#e3f2fd;
#         padding:15px;
#         border-radius:10px;
#         margin-top:20px;
#         text-align:center;
#         font-size:18px;
#         color:#2e7d32;
#         font-weight:bold;
#         box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
#     ">
#         üèÜ Best Model: <span style="color:#2e7d32;">{best_model_name}</span>
#         (Accuracy = {accuracies[best_model_name]:.2f})
#     </div>
#     """,
#     unsafe_allow_html=True
# )
# # -------------------- Dataset Preview --------------------
# st.subheader("üßæ Dataset Preview")
# 
# with st.expander("üìÇ View Raw Dataset", expanded=False):
#     st.write("Here‚Äôs a preview of the dataset used for model training:")
#     st.dataframe(df.head(10))  # Show first 10 rows
#     st.write(f"**Total Records:** {df.shape[0]} | **Total Features:** {df.shape[1]}")
# 
# # -------------------- Dataset Preview (Cleaned) --------------------
# st.subheader("üßæ Cleaned Dataset Preview")
# df_cleaned = df.drop_duplicates().dropna().copy() # Clean dataset (remove duplicates/missing values)
# with st.expander("üìÇ View Cleaned Dataset (Numeric)", expanded=True):
#     st.dataframe(df_cleaned)  # Show numeric values
#     st.write(f"**Total Records:** {df_cleaned.shape[0]} | **Total Features:** {df_cleaned.shape[1]}")
# 
# # -------------------- Dataset Preview (String Format) --------------------
# df_string = df_cleaned.copy()
# #Reverse mapping: numeric ‚Üí string
# reverse_mapping = {
#     1: "Never",
#     2: "Always",
#     3: "Often",
#     4: "Rarely",
#     5: "Sometimes",
#     6: "Not at all"
# }
# 
# # Apply mapping to all trained_features columns
# for col in trained_features:
#     if pd.api.types.is_numeric_dtype(df_string[col]):
#         df_string[col] = df_string[col].map(reverse_mapping)
# 
# with st.expander("üìÇ View Cleaned Dataset (String Format)", expanded=True):
#     st.dataframe(df_string)  # Show string values
#     st.write(f"**Total Records:** {df_string.shape[0]} | **Total Features:** {df_string.shape[1]}")

import subprocess
subprocess.Popen(["streamlit", "run", "/content/streamlitapp.py", "--server.port", "8501", "--server.address", "0.0.0.0"])

!ngrok config add-authtoken "333qEsi4bxkebuFN1SWcsA8McYI_6pJKPmrzFqX5Yjr7NYtPi"

from pyngrok import ngrok
import os

# Expose Streamlit port 8501
public_url = ngrok.connect(8501)
print("üåç Public URL:", public_url)

# Run Streamlit app (in the background)
!streamlit run your_app.py &>/dev/null &